{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theodore/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/theodore/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "style.use(\"ggplot\")\n",
    "from textwrap import wrap\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggin face dataset\n",
    "dataset1 = load_dataset(\"artem9k/ai-text-detection-pile\")\n",
    "\n",
    "df = pd.DataFrame(dataset1['train'].to_pandas())\n",
    "df = df[['text','source']]\n",
    "df['source'] = df['source'].map({'human': 0, 'ai': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using m1 mac\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text  source\n",
      "0        12 Years a Slave: An Analysis of the Film Essa...       0\n",
      "1        20+ Social Media Post Ideas to Radically Simpl...       0\n",
      "2        2022 Russian Invasion of Ukraine in Global Med...       0\n",
      "3        533 U.S. 27 (2001) Kyllo v. United States: The...       0\n",
      "4        A Charles Schwab Corporation Case Essay\\n\\nCha...       0\n",
      "...                                                    ...     ...\n",
      "1392517  Today, I accomplished a major feat. I stepped ...       1\n",
      "1392518  As rockets rain down from the sky\\nEurope trem...       1\n",
      "1392519  On January 6th, 2023, the world lost a true pi...       1\n",
      "1392520  A gene bank is a repository of genetic materia...       1\n",
      "1392521  On the twelfth day of Christmas, My true love ...       1\n",
      "\n",
      "[1392522 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 835513\n",
      "Validation set size: 278504\n",
      "Test set size: 278505\n"
     ]
    }
   ],
   "source": [
    "# 60% Training, 20% Validation, 20% Test\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  \n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  \n",
    "\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Validation set size:\", len(valid_df))\n",
    "print(\"Test set size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/v74xm1tn6zn5xx52z3f4_9b40000gn/T/ipykernel_4947/1483335269.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df0_text = train_df[df['source'] == 0].copy()\n",
      "/var/folders/7_/v74xm1tn6zn5xx52z3f4_9b40000gn/T/ipykernel_4947/1483335269.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df1_ai = train_df[df['source'] == 1].copy()\n"
     ]
    }
   ],
   "source": [
    "df0_text = train_df[train_df['source'] == 0].copy()\n",
    "df1_ai = train_df[train_df['source'] == 1].copy()\n",
    "\n",
    "train_df = pd.concat([df0_text,df1_ai], ignore_index=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish stop words and linking words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "linking_words = {'to', 'the', 'and', 'of', 'in', 'on', 'for', 'with', 'at', 'a', 'an'}\n",
    "\n",
    "# sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in linking_words]\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return len(tokens), sentiment_scores['compound']\n",
    "\n",
    "df0_text[['text_length', 'sentiment_score']] = df0_text['text'].apply(lambda x: pd.Series(analyze_text(x)))\n",
    "df1_ai[['text_length', 'sentiment_score']] = df0_text['text'].apply(lambda x: pd.Series(analyze_text(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
